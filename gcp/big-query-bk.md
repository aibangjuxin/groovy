ä¸ºäº†å¤‡ä»½ Google BigQuery æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•ã€‚è¿™é‡Œæˆ‘ä¼šç»™å‡ºä¸€ä¸ªå®Œæ•´çš„å¤‡ä»½æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¯èƒ½æ¶‰åŠçš„æ­¥éª¤å’Œå‘½ä»¤ã€‚

### æ–¹æ¡ˆæ¦‚è¿°

1. **å¯¼å‡ºæ•°æ®åˆ°Google Cloud Storage (GCS)**
2. **å°†å¯¼å‡ºçš„æ•°æ®ä»GCSä¸‹è½½åˆ°æœ¬åœ°æˆ–å…¶ä»–å­˜å‚¨**
3. **å®šæœŸè‡ªåŠ¨åŒ–å¤‡ä»½**

### è¯¦ç»†æ­¥éª¤

#### 1. å¯¼å‡ºæ•°æ®åˆ°Google Cloud Storage (GCS)

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†BigQueryçš„æ•°æ®å¯¼å‡ºåˆ°GCSã€‚å¯ä»¥ä½¿ç”¨BigQueryå‘½ä»¤è¡Œå·¥å…· (`bq`) æˆ–è€… BigQuery API æ¥æ‰§è¡Œè¿™ä¸€æ­¥ã€‚

**ä½¿ç”¨bqå‘½ä»¤è¡Œå·¥å…·ï¼š**

```bash
bq extract --destination_format=CSV 'project_id:dataset.table' gs://your_bucket/backup_filename.csv
```

**ä½¿ç”¨Pythonå®¢æˆ·ç«¯åº“ï¼š**

```python
from google.cloud import bigquery
import datetime

# åˆ›å»ºBigQueryå®¢æˆ·ç«¯
client = bigquery.Client()

# è®¾ç½®ç›®æ ‡è¡¨
project = 'your_project_id'
dataset_id = 'your_dataset_id'
table_id = 'your_table_id'
table_ref = client.dataset(dataset_id).table(table_id)

# è®¾ç½®å¯¼å‡ºé…ç½®
destination_uri = 'gs://your_bucket/backup_filename.csv'
extract_job = client.extract_table(
    table_ref,
    destination_uri,
    # å¯é€‰å‚æ•°
    location='US'  # é¡¹ç›®ä½ç½®
)

# ç­‰å¾…ä½œä¸šå®Œæˆ
extract_job.result()
print("å¯¼å‡ºå®Œæˆ")
```

#### 2. å°†å¯¼å‡ºçš„æ•°æ®ä»GCSä¸‹è½½åˆ°æœ¬åœ°æˆ–å…¶ä»–å­˜å‚¨

**ä½¿ç”¨gsutilå‘½ä»¤è¡Œå·¥å…·ï¼š**

```bash
gsutil cp gs://your_bucket/backup_filename.csv /local/directory/backup_filename.csv
```

#### 3. å®šæœŸè‡ªåŠ¨åŒ–å¤‡ä»½

ä¸ºäº†å®ç°å®šæœŸå¤‡ä»½ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

- **ä½¿ç”¨Cloud Schedulerå’ŒCloud Functionsï¼š** è®¾ç½®ä¸€ä¸ªå®šæ—¶ä»»åŠ¡è§¦å‘Cloud Functionæ¥æ‰§è¡Œå¤‡ä»½ã€‚
- **ä½¿ç”¨æœ¬åœ°è„šæœ¬å’Œcron jobsï¼š** æœ¬åœ°æœåŠ¡å™¨ä¸Šçš„cron jobæ¥å®šæœŸè¿è¡Œå¤‡ä»½è„šæœ¬ã€‚

**ç¤ºä¾‹ï¼šä½¿ç”¨Cloud Schedulerå’ŒCloud Functions**

1. **åˆ›å»ºCloud Function**

```python
import google.auth
from google.cloud import bigquery
from google.cloud import storage

def backup_bigquery_data(event, context):
    credentials, project = google.auth.default()
    client = bigquery.Client(credentials=credentials, project=project)

    dataset_id = 'your_dataset_id'
    table_id = 'your_table_id'
    destination_uri = 'gs://your_bucket/backup_filename_{}.csv'.format(datetime.datetime.now().strftime("%Y%m%d%H%M%S"))

    table_ref = client.dataset(dataset_id).table(table_id)
    extract_job = client.extract_table(table_ref, destination_uri)

    extract_job.result()
    print('Backup completed to {}'.format(destination_uri))
```

2. **éƒ¨ç½²Cloud Function**

```bash
gcloud functions deploy backup_bigquery_data --runtime python39 --trigger-http --allow-unauthenticated
```

3. **è®¾ç½®Cloud Scheduler**

```bash
gcloud scheduler jobs create http bigquery-backup-job --schedule="0 2 * * *" \
  --uri=https://REGION-PROJECT_ID.cloudfunctions.net/backup_bigquery_data \
  --http-method=GET
```

### æ€»ç»“

è¿™ä¸ªå¤‡ä»½æ–¹æ¡ˆé€šè¿‡å¯¼å‡ºBigQueryæ•°æ®åˆ°GCSå¹¶å®šæœŸæ‰§è¡Œè‡ªåŠ¨åŒ–å¤‡ä»½ï¼Œç¡®ä¿æ•°æ®çš„å®‰å…¨æ€§å’Œå¯ç”¨æ€§ã€‚ä½ å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´è„šæœ¬å’Œé…ç½®ã€‚å¸Œæœ›è¿™ä¸ªæ–¹æ¡ˆèƒ½å¸®åˆ°ä½ ï¼ğŸ’¡