#!/bin/bash
set -euo pipefail

# Define descriptive variable names
readonly project_id="your_project_id"
readonly view_file="view.json"
readonly schema_file="schema.txt"
true > $view_file
true > $schema_file
# Define function to print info messages
readonly script_name="${0##*/}"
info() {
        echo "${script_name}: ${1}"
}

# Create temporary directory
readonly temp_dir=$(mktemp -d)

# Define cleanup function
cleanup() {
        rm -rf "$temp_dir"
}
trap cleanup EXIT

# define a single table you can edit this one
bigquerytable="api_data"
#bigquerytable="my_dataset.my_table"

echo $bigquerytable
# Activate project configuration
info "Activating project configuration"
gcloud config configurations activate "$project_id" && gcloud config set project "$project_id"

# Check if required commands are available
if ! command -v bq >/dev/null 2>&1; then
        info "bq command is not found"
        info "Please install bq for your environment"
        exit 1
fi

if ! command -v jq >/dev/null 2>&1; then
        info "jq command is not found"
        exit 1
fi

# Verify $bigquerytable dataset
if ! bq ls --project_id "$project_id" | grep -q "$bigquerytable"; then
        echo "$bigquerytable not found"
        exit 1
fi

# List all datasets in the project
info "Listing all datasets in the project"
for dataset in $(bq ls --project_id "$project_id" --format=json | jq -r '.[].datasetReference.datasetId'); do
        info "Processing dataset $dataset"
        # List all views in the dataset
        #bq ls --project_id "$project_id" --format=json "$dataset" | jq -r '.[] | select(.type == "VIEW") | .tableReference.tableId' | while read -r view; do
        #       info "Processing view $view"
        #       bq query --project_id "$project_id" --format=json "SELECT * FROM $dataset.$view LIMIT 0" | jq -r '{tableId: .configuration.view.tableId, query: .configuration.view.query}' >>"$temp_dir/$view_file"
        #done
done

info "will verify $bigquerytable tables"
bq ls --format=pretty "$project_id":"$bigquerytable"
# List all views in the dataset
info "The next will only print view tables "
bq ls --format=pretty $project_id:"$bigquerytable" | grep VIEW |awk -F"|" '{print$2}'|sed 's/^[ \t]*//g' | while read view; do
	echo $view
	bq show --format=prettyjson $project_id:"$bigquerytable"."$view" | jq '{tableId: .tableReference.tableId, query: .view.query}' >>$view_file
done

# Export schema for all tables in $bigquerytable dataset
info "Exporting schema for all tables in $bigquerytable dataset"
bq ls --format=pretty $project_id:$bigquerytable | grep -v "tableId" | awk -F"|" '{print$2}'|sed '/^$/d'|sed 's/^[ \t]*//g' | while read table; do
	echo "------$table-------" >>$schema_file
	bq show --format=prettyjson $project_id:$bigquerytable."$table" | jq -r '.schema.fields[].name' >>$schema_file
done


info "Schema exported to $schema_file"

#!/bin/bash
# define project id bigquery

# Define function to print info messages
SCRIPT_NAME="${0##*/}"
info() {
	echo "${SCRIPT_NAME}: ${1}"
}
# Define file directory
info "Defining file directory"
current_directory=$(
	cd "$(dirname "$0")" || exit
	pwd
)
echo "The working directory is: $current_directory"

# Create temp directory
mkdir -pv "$current_directory"/tempbq
tmpdir=$(
	cd "$current_directory"/tempbq || exit
	pwd
)
cd "$tmpdir" || exit

# Define output files
viewfile="view.json"
true >$viewfile

schemafile="schema.txt"
true >$schemafile
# Define project ID
project="your_project_id"

# Activate project configuration
gcloud config configurations activate "$project" && gcloud config set project "$project"
# Verify the command exists
if command -v bq >/dev/null 2>&1; then
	echo "The command bq is exists will continue"
else
	echo "no exists bq"
	echo "please install bq for you env"
	exit 1
fi

if ! command -v jq &>/dev/null; then
	echo "jq could not be found"
	exit 1
fi

if ! [ -x "$(command -v jq)" ]; then
	echo 'Error: jq is not installed.' >&2
	exit 1
fi

#jsonformat=$(command -v jq)
# replace jq to $jsonformat

# List all datasets in the project
info "will list all of the datasets in the project"
bq ls --project_id $project --format=json | jq -r '.[].datasetReference.datasetId' | while read dataset; do
	echo $dataset
done

# define a single table you can edit this one
bigquerytable="api_data"
#bigquerytable="my_dataset.my_table"

# Verify $bigquerytable tables
if bq ls --project_id $project | grep -q "$bigquerytable"; then
	info "will verify $bigquerytable tables"
	bq ls --format=pretty $project:$bigquerytable
	info "The next will only print view tables "
	bq ls --format=pretty $project:$bigquerytable | grep VIEW |awk -F"|" '{print$2}'|sed 's/^[ \t]*//g' | while read view; do
		echo $view
		bq show --format=prettyjson $project:$bigquerytable."$view" | jq '{tableId: .tableReference.tableId, query: .view.query}' >>$viewfile
	done

	# Export schema for different datasets
	bq ls --format=pretty $project:$bigquerytable | grep -v "tableId" | awk -F"|" '{print$2}'|sed '/^$/d'|sed 's/^[ \t]*//g' | while read table; do
		echo "----------$table----------" >>$schemafile
		bq show --format=prettyjson $project:$bigquerytable."$table" | jq -r '.schema.fields[].name' >>$schemafile
	done
else
	echo "$bigquerytable not found"
	exit 1
fi